{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83c\udfe0 \ud83d\udee0 Home Operations","text":"<p>Home Operations repository for managing my Homelab infrastructure.</p> <p> </p>"},{"location":"#overview","title":"\ud83d\udcdd Overview","text":"<p>Note</p> <p>This repository is a constant work in progress and I will continue to update it as I learn more.</p> <p>The goals of this repository are:</p> <ul> <li>Automate the configuration of my Homelab infrastructure and deployment of applications.</li> <li>Adhere to best practices.</li> <li>Learn and test new technologies and concepts.</li> <li>Document my Homelab setup and configuration for future reference and in case of disaster recovery.</li> <li>Share knowledge and learnings with others.</li> </ul>"},{"location":"#ansible-content","title":"Ansible Content","text":"<p>Ansible content used to configure my Homelab infrastructure and deploy applications are located in the ansible directory.</p>"},{"location":"#license","title":"License","text":"<p>MIT</p>"},{"location":"docker-compose/","title":"Docker Compose","text":"<p>Docker Compose is used to deploy certain applications in my Homelab. The docker directory contains subdirectories for each application.</p>"},{"location":"docker-compose/#environment-variables","title":"Environment Variables","text":"<p>Each directory contains a <code>.env</code> file which references secrets from a 1Password Vault. On the Docker host, the <code>op</code> CLI is configured to authenticate with a 1Password Service Account. The secret references are then substituted at runtime when using the <code>op run</code> command.</p>"},{"location":"docker-compose/#deployment","title":"Deployment","text":"<p>Info</p> <p>The example commands below are for Caddy, but the process is identical for the other application directories.</p> <ol> <li> <p>Copy the application directory to the Docker host:</p> <pre><code>scp -r caddy daniel@pihole02.net.dbren.uk:~/\n</code></pre> </li> <li> <p>SSH to the Docker host:</p> <pre><code>ssh daniel@pihole02.net.dbren.uk\n</code></pre> </li> <li> <p>Deploy the application:</p> <pre><code>cd caddy &amp;&amp; op run --env-file=./.env -- docker compose up -d\n</code></pre> </li> </ol>"},{"location":"ansible/awx/","title":"AWX","text":"<p>AWX is used in my Homelab to run Ansible content against devices.</p> <p>AWX is deployed via the AWX Operator on Kubernetes. I'm running version <code>2.19.1</code> of the operator.</p> <p>I have a single node K3s VM on my Proxmox VE node which I deployed using OpenTofu. The K3s deployment is done via an Ansible Playbook.</p> <p>The awx-on-k3s project is used to deploy the AWX Operator and AWX Custom Resource Definition (CRD) on K3s . I use an Ansible playbook to prepare the K3s node for AWX deployment.</p>"},{"location":"ansible/awx/#deployment","title":"Deployment","text":"<ol> <li> <p>Run the <code>playbook-awx-deploy.yml</code> Ansible playbook:</p> <pre><code>ansible-playbook playbooks/playbook-awx-deploy.yml\n</code></pre> </li> <li> <p>Deploy AWX:</p> <pre><code>ansible -b k3s01.net.dbren.uk -m command -a 'kubectl apply -k awx-on-k3s/base/'\n</code></pre> </li> </ol>"},{"location":"ansible/awx/#configuration","title":"Configuration","text":"<p>An Ansible playbook is used to configure AWX with the Execution Environment, credentials, project, inventories and Discord notification template.</p>"},{"location":"ansible/execution-environment/","title":"Execution Environment","text":"<p>An Ansible Execution Environment (EE) is used to run Ansible content against devices in my Homelab.</p> <p>The EE is built using <code>ansible-builder</code>.</p>"},{"location":"ansible/execution-environment/#execution-environment-files","title":"Execution Environment Files","text":"<p>Files relating to the Execution Environment are located in <code>ansible/ee</code>.</p> File Path Description <code>ansible/ee/execution-environment.yml</code> Configuration file used by <code>ansible-builder</code> to create the EE. <code>ansible/ee/requirements.txt</code> Extra Python dependencies to include in the EE. <code>ansible/ee/requirements.yml</code> Ansible collection and roles to include in the EE. <code>ansible/ee/custom_entrypoint.sh</code> Entrypoint script used to configure specific environments variables for the 1Password CLI and SSH agent socket."},{"location":"ansible/execution-environment/#secrets","title":"Secrets","text":"<p>The 1Password CLI is installed in the EE to retrieve secrets for devices and services.</p>"},{"location":"ansible/execution-environment/#automated-build","title":"Automated Build","text":"<p>A GitHub Action is set up to automatically re-build the EE when changes are made to files in <code>ansible/ee</code>.</p>"},{"location":"ansible/execution-environment/#using-the-execution-environment","title":"Using the Execution Environment","text":""},{"location":"ansible/execution-environment/#ansible-navigator","title":"Ansible Navigator","text":"<p><code>ansible-navigator</code> is used on my Macbook M1 Pro Max to run Ansible Content against devices in my Homelab. Under the hood <code>ansible-navigator</code> uses <code>ansible-runner</code> to interact with the container engine to launch the EE. I use OrbStack which has a compatible Docker engine.</p> <p><code>ansible-navigator</code> is configured using the <code>ansible-navigator.yml</code> file. I use specific configuration so that the EE can access the 1Password SSH Agent running on my Macbook to connect to devices. Furthermore, as mentioned above the EE has the 1Password CLI installed which is used by the <code>community.general.onepassword</code> lookup plugin to retrieve secrets from a 1Password vault.</p>"},{"location":"ansible/execution-environment/#awx","title":"AWX","text":"<p>The EE is used by my AWX instance to run Ansible Content against devices in my Homelab. See AWX for more information.</p>"},{"location":"ansible/minecraft/","title":"Minecraft","text":"<p>Archived</p> <p>This page has been archived and kept for reference. Some of the links on this page may no longer work.</p> <p>The Minecraft playbook is used to deploy Minecraft servers on Ubuntu Server 22.04 LTS in my Homelab.</p>"},{"location":"ansible/minecraft/#ansible-playbook","title":"Ansible Playbook","text":"<p>The playbook configures two Minecraft servers, <code>minecraft01</code> and <code>minecraft02</code>; each with different configuration. Both Minecraft servers are deployed using the itzg/minecraft-server container image.</p>"},{"location":"ansible/minecraft/#vanilla-server","title":"Vanilla Server","text":"<p>The <code>minecraft01</code> server is deployed with Paper MC. Server specific settings are located in <code>ansible/vars/paper_minecraft.yml</code>.</p>"},{"location":"ansible/minecraft/#modded-server","title":"Modded Server","text":"<p>The <code>minecraft02</code> server is deployed with the All the Mods 9 (ATM9) modpack. Server specific settings are located in <code>ansible/vars/modded_minecraft.yml</code>.</p>"},{"location":"ansible/minecraft/#staging-the-modpack-server-zip-file","title":"Staging the Modpack Server ZIP File","text":"<p>To run ATM9 on the <code>minecraft02</code> server, the modpack server ZIP file must be staged on the server prior to running the Ansible playbook.</p> <ol> <li> <p>Download the modpack server ZIP file from CurseForge.</p> </li> <li> <p>SCP the modpack server ZIP file to <code>minecraft02</code>:</p> <pre><code>ssh daniel@minecraft02.net.dbren.uk mkdir -pv ~/modpacks\nscp /path/to/Server-Files-0.2.41.zip minecraft02.net.dbren.uk:~/modpacks/\n</code></pre> </li> <li> <p>Update the <code>ansible/vars/modded_minecraft.yml</code> file with the correct modpack server ZIP file name:</p> <pre><code>minecraft_options:\n  CF_SERVER_MOD: /modpacks/Server-Files-0.2.41.zip\n</code></pre> </li> </ol>"},{"location":"ansible/minecraft/#server-files-world-backup","title":"Server Files &amp; World Backup","text":"<p>The Ansible playbook is configured to deploy the itzg/mc-backup container image which will backup the Minecraft server files and world to a Backblaze B2 S3 bucket. This occurs every 24 hours.</p> <p>See dbrennand | home-ops Backblaze for more information on how to configure the Backblaze B2 S3 bucket.</p>"},{"location":"ansible/minecraft/#itzgmc-backup-removing-stale-locks","title":"itzg/mc-backup - Removing Stale Locks","text":"<p>You may come across the following error in the logs. This occurs when the server is shut down unexpectedly during a backup and the restic lock file is not removed:</p> <pre><code>docker logs minecraft-backup\n# ...\nERROR the `unlock` command can be used to remove stale locks\n</code></pre> <p>Remove the lock file by running <code>restic unlock</code>:</p> <pre><code>docker restart minecraft-backup; docker exec -it minecraft-backup restic -r b2:&lt;bucket name&gt; unlock\n</code></pre>"},{"location":"ansible/minecraft/#tailscale","title":"Tailscale","text":"<p>Tailscale is used on the server to allow friends to connect to the server remotely.</p>"},{"location":"ansible/minecraft/#opentofu","title":"OpenTofu","text":"<p>Both servers are deployed using OpenTofu.</p>"},{"location":"ansible/minecraft/#usage","title":"Usage","text":"<ol> <li> <p>Clone the repository:</p> <pre><code>git clone https://github.com/dbrennand/home-ops.git &amp;&amp; cd home-ops/ansible\n</code></pre> </li> <li> <p>Create the Python virtual environment and install Ansible dependencies:</p> <pre><code>task venv\ntask ansible:requirements\n</code></pre> </li> <li> <p>Verify Ansible can connect to the server:</p> <pre><code>task ansible:adhoc -- minecraft -m ping\n</code></pre> </li> <li> <p>Run the playbook:</p> <pre><code>task ansible:play -- playbooks/minecraft-playbook.yml\n# The following tags are supported: minecraft, backup\n# Example using tags:\ntask ansible:play -- playbooks/minecraft-playbook.yml --tags minecraft\n</code></pre> </li> </ol>"},{"location":"infrastructure/backblaze/","title":"Backblaze Object Storage (S3)","text":"<p>Backblaze B2 Cloud Storage is used in my Homelab to store backups and provide object storage for various applications.</p>"},{"location":"infrastructure/backblaze/#backblaze-b2-cli","title":"Backblaze B2 CLI","text":"<p>The Backblaze B2 CLI is used to create buckets, upload and download files, and manage your account.</p> <p>Install the Backblaze B2 CLI on MacOS:</p> <pre><code>brew install b2-tools\n</code></pre>"},{"location":"infrastructure/backblaze/#creating-a-backblaze-b2-bucket","title":"Creating a Backblaze B2 Bucket","text":"<p>Create a new bucket keeping only the last version of files:</p> <pre><code>BUCKET_NAME=\"my-bucket\"\nb2 bucket create \"${BUCKET_NAME}\" allPrivate\n# To keep only the last version of files\nb2 bucket create --lifecycle-rule '{\"daysFromHidingToDeleting\": 1, \"daysFromUploadingToHiding\": null, \"fileNamePrefix\": \"\"}' \"${BUCKET_NAME}\" allPrivate\n</code></pre>"},{"location":"infrastructure/backblaze/#creating-application-keys","title":"Creating Application Keys","text":"<p>Create a new application key with specific permissions for a bucket:</p> <pre><code>BUCKET_NAME=\"my-bucket\"\nKEY_NAME=\"my-key\"\nb2 key create --bucket \"${BUCKET_NAME}\" \"${KEY_NAME}\" readFiles,writeFiles,listFiles,deleteFiles,readBuckets,listBuckets\n</code></pre>"},{"location":"infrastructure/beszel/","title":"Beszel","text":"<p>Beszel is a simple, lightweight server monitoring solution that I use to monitor all my Homelab devices.</p> <p>The Beszel Hub is hosted on a VPS deployed on Hetzner Cloud. The Beszel Hub is deployed using Docker Compose using the method documented here.</p> <p></p> <p>Each Homelab device has the Beszel binary agent which communicates system metrics to the Hub. I deploy this agent using the <code>community.beszel.agent</code> Ansible role which I run from an Ansible playbook <code>playbook-beszel-agent.yml</code>.</p>"},{"location":"infrastructure/beszel/#tailscale-sidecar","title":"Tailscale Sidecar","text":"<p>Alongside the Beszel Hub is a Tailscale sidecar container which allows the Hub to communicate with devices on my home network, and only be accessible over Tailscale using Tailscale serve.</p> <p>The sidecar container accepts my Tailnet's configured DNS servers and accepts subnet routes. This is what allows the Beszel Hub to communicate with devices on my home network.</p>"},{"location":"infrastructure/beszel/#discord-webhook-notifications","title":"Discord Webhook Notifications","text":"<p>The Beszel Hub is configured to send notifications into a Discord channel when certain system thresholds are breached for over 10 minutes.</p> <pre><code>generic://discord.com/api/webhooks/...?template=json&amp;messagekey=content\n</code></pre>"},{"location":"infrastructure/dns/","title":"DNS","text":"<p>NextDNS is the upstream DNS provider in my Homelab.</p> <p>What is NextDNS?</p> <p>NextDNS protects you from all kinds of security threats, blocks ads and trackers on websites and in apps.</p>"},{"location":"infrastructure/dns/#rewrites-a-records","title":"Rewrites (A Records)","text":"<p>NextDNS has a rewrites feature which allows me to create DNS A records for my Homelab. I manage these rewrite records via the NextDNS REST API using a script I created called NextDNS-Rewrites. My configuration file is located here.</p>"},{"location":"infrastructure/hetzner/","title":"Hetzner Cloud","text":"<p>Hetzner Cloud is a European based public cloud provider. Based in Germany, they also have datacenters in Finland, USA and Singapore.</p>"},{"location":"infrastructure/hetzner/#cloud-vps","title":"Cloud VPS","text":"<p>I have a single cloud VPS provisioned on Hetzner. The VPS is deployed using OpenTofu using the method documented here.</p> <p>I'm currently using this VPS for:</p> <ul> <li>Remotely monitoring via Tailscale all my Homelab devices using Beszel.</li> <li>Hosting a status page using Tailscale Funnel. All devices are accessed remotely via Tailscale.</li> </ul>"},{"location":"infrastructure/media/","title":"Media Server","text":"<p>Media server for hosting files and various containerised services.</p> <p>What is OpenMediaVault?</p> <p>OpenMediaVault is the next generation network attached storage (NAS) solution based on Debian Linux. It contains services like SSH, (S)FTP, SMB/CIFS, RSync and many more ready to use.</p>"},{"location":"infrastructure/media/#deployment","title":"Deployment","text":"<p>The media server is deployed as a VM on Proxmox using the OpenMediaVault ISO.</p> <ol> <li> <p>Download the ISO to the Node 1 storage.</p> </li> <li> <p>Navigate to <code>proxmox01</code> &gt; <code>Create VM</code>.</p> </li> <li> <p>Provide the following details for <code>General</code> and click Next:</p> Setting Value Name <code>media01</code> Node <code>proxmox01</code> Start at boot \u274c </li> <li> <p>Under <code>OS</code>, select the storage where the ISO was downloaded to and choose the Proxmox Backup Server ISO image. Click Next.</p> </li> <li> <p>Under <code>System</code>, select the <code>VirtIO SCSI Single</code> controller and click Next.</p> </li> <li> <p>Provide the following details for <code>Disks</code> and click Next:</p> Setting Value Bus/Device <code>SCSI</code> Storage <code>local-lvm</code> Size <code>50GiB</code> Format <code>Raw disk image (raw)</code> Discard \u2705 SSD Emulation \u2705 Setting Value Bus/Device <code>SCSI</code> Storage <code>lv-ssd-samsung</code> Size <code>930GiB</code> Format <code>Raw disk image (raw)</code> Discard \u2705 SSD Emulation \u2705 Setting Value Bus/Device <code>SCSI</code> Storage <code>lv-ssd-crucial</code> Size <code>50GiB</code> Format <code>Raw disk image (raw)</code> Discard \u2705 SSD Emulation \u2705 </li> <li> <p>Provide the following details for <code>CPU</code> and click Next:</p> Setting Value Cores <code>4</code> Type <code>host</code> </li> <li> <p>Provide the following details for <code>Memory</code> and click Next:</p> Setting Value Memory (MiB) <code>8192</code> Ballooning Device \u2705 Minimum Memory <code>1024</code> </li> <li> <p>Leave <code>Network</code> as default, click Next and confirm deployment.</p> </li> <li> <p>Start the <code>media01</code> VM and open the console to begin the installation.</p> </li> <li> <p>Follow the on-screen instructions to install OpenMediaVault, when prompted enter the following details:</p> Setting Value Hostname <code>media01</code> Domain Name <code>net.dbren.uk</code> Email Enter email Password Enter password Default Gateway <code>192.168.0.1</code> Subnet Mask <code>255.255.255.0</code> IP Address <code>192.168.0.9</code> </li> <li> <p>Once installation has completed, login to the web interface using the FQDN and credentials entered during installation.</p> </li> </ol>"},{"location":"infrastructure/media/#post-installation","title":"Post Installation","text":"<p>Info</p> <p>Where required, make sure to apply changes before moving on to the next step. A yellow box will appear after certain operations if this is necessary.</p> <ol> <li> <p>Navigate to <code>System</code> &gt; <code>Date &amp; Time</code> and set the time zone to <code>Europe/London</code>.</p> </li> <li> <p>Under <code>Storage</code> &gt; <code>File Systems</code>, click the <code>+</code> symbol and use the following configuration, repeating for each file system:</p> Setting Value File System <code>EXT4</code> Device <code>/dev/sdb</code> Label <code>apps</code> Setting Value File System <code>EXT4</code> Device <code>/dev/sdc</code> Label <code>paperless-ngx</code> <p>Click Save when finished.</p> <p>The file system configuration should look like below:</p> <p></p> </li> <li> <p>Navigate to <code>Storage</code> &gt; <code>Shared Folders</code> and click the <code>+</code> symbol and use the following configuration, repeating for each shared folder:</p> Setting Value Name <code>apps</code> Device <code>/dev/sdb1</code> Permissions <code>Administrator: read/write, Users: read/write, Others: read-only</code> Relative Path <code>apps/</code> Tags <code>apps</code> Setting Value Name <code>consume</code> Device <code>/dev/sdc1</code> Permissions <code>Everyone: read/write</code> Relative Path <code>consume/</code> Tags <code>consume</code>, <code>paperless-ngx</code> Setting Value Name <code>media</code> Device <code>/dev/sdc1</code> Permissions <code>Administrator: read/write, Users: read/write, Others: read-only</code> Relative Path <code>media/</code> Tags <code>apps</code> Tags <code>media</code>, <code>paperless-ngx</code> <p>The shared folder configuration should look like below:</p> <p></p> </li> <li> <p>Navigate to <code>Services</code> &gt; <code>SSH</code> and configure the following settings:</p> Setting Value Password Authentication \u274c Public Key Authentication \u2705 </li> </ol>"},{"location":"infrastructure/media/#smbcifs","title":"SMB/CIFS","text":"<p>The steps below can be used to create a SMB/CIFS share for the Paperless-ngx consume directory.</p> <ol> <li> <p>Navigate to <code>Services</code> &gt; <code>SMB/CIFS</code> &gt; <code>Settings</code> and configure the following settings:</p> Setting Value Enabled \u2705 Browsable \u2705 Minimum protocol version <code>SMB3</code> </li> <li> <p>Navigate to <code>Services</code> &gt; <code>SMB/CIFS</code> &gt; <code>Shares</code> and click the <code>+</code> symbol to add a new share with the following configuration:</p> Setting Value Enabled \u2705 Shared Folder <code>consume [consume, paperless-ngx]</code> Comment <code>Paperless-ngx consume directory</code> Public Guests only Browsable \u2705 Inherit ACLs \u2705 Inherit Permissions \u2705 <p>The share configuration should look like below:</p> <p></p> </li> </ol>"},{"location":"infrastructure/opentofu/","title":"OpenTofu","text":"<p>OpenTofu (a fork of Terraform) is used to deploy infrastructure in my Homelab. Resources located in <code>terraform</code> are used to deploy VMs on my Proxmox VE node.</p>"},{"location":"infrastructure/opentofu/#custom-modules","title":"Custom Module(s)","text":"<p>My Home-Ops project contains three OpenTofu modules I've created. The only active one is <code>proxmox_cloud_init_config</code> which I use to create a Cloud-init configuration file for each VM. I previously used the <code>proxmox_vm</code> module but I've found that it doesn't provide enough flexibility for me anymore so I've deprecated it.</p>"},{"location":"infrastructure/opentofu/#opentofu-state","title":"OpenTofu State","text":"<p>The OpenTofu state is stored in a Backblaze S3 bucket.</p>"},{"location":"infrastructure/opentofu/#secrets","title":"Secrets","text":"<p>The 1Password Terraform provider is used to retrieve credentials for Proxmox and an SSH key used during VM creation.</p>"},{"location":"infrastructure/opentofu/#usage","title":"Usage","text":"<ol> <li> <p>Install OpenTofu:</p> <pre><code>brew install opentofu\n</code></pre> </li> <li> <p>Initialize OpenTofu providers and the S3 backend:</p> <pre><code>cd terraform\nop run --env-file=./.env -- tofu init\n</code></pre> </li> <li> <p>Plan the deployment:</p> <pre><code>op run --env-file=./.env -- tofu plan\n</code></pre> </li> <li> <p>If everything looks good, apply the deployment:</p> <pre><code>op run --env-file=./.env -- tofu apply\n</code></pre> </li> </ol>"},{"location":"infrastructure/opentofu/#destroying-specific-resources","title":"Destroying Specific Resources","text":"<p>To destroy specific resources, use the <code>tofu destroy</code> command with the <code>-target</code> flag:</p> <pre><code>op run --env-file=./.env -- tofu destroy -target module.proxmox_vm_control01 -target module.proxmox_vm_worker01 -target module.proxmox_vm_worker02\n</code></pre>"},{"location":"infrastructure/opentofu/#removing-state-for-manually-destroyed-resources","title":"Removing State for Manually Destroyed Resources","text":"<p>If resources are manually destroyed, the state file will need to be updated to reflect the changes to the infrastructure. To do this, use the <code>tofu state rm</code> command:</p> <pre><code>op run --env-file=./.env -- tofu state rm 'module.proxmox_vm_worker02'\n</code></pre>"},{"location":"infrastructure/raspberrypi3/","title":"Raspberry Pi 3","text":"<p>Archived</p> <p>This page has been archived and kept for reference. Some of the links on this page may no longer work.</p> <p>A Raspberry Pi 3 is used in my Homelab as a Tailscale subnet router and exit node.</p>"},{"location":"infrastructure/raspberrypi3/#raspberry-pi-3-os-setup","title":"Raspberry Pi 3 OS Setup","text":"<ol> <li> <p>Flash Raspberry Pi OS Lite (64-bit) to a micro SD card using Raspberry Pi Imager.</p> </li> <li> <p>Click the cog icon on the bottom right to open the Advanced options menu and configure the following options:</p> <ul> <li>Set hostname to <code>subnet01</code>.</li> <li>Enable SSH and allow public key authentication only. Enter your public key in the <code>authorized_keys</code> field.</li> <li>Set a username and password.</li> <li>Set locale settings as appropriate.</li> <li>Disable telemetry.</li> </ul> </li> <li> <p>Once flashed, insert the micro SD card into the Raspberry Pi and boot it.</p> </li> <li> <p>SSH to the Raspberry Pi:</p> <p>Tip</p> <p>If you experience issues with hostname resolution on MacOS, try restarting the mDNSResponder service:</p> <pre><code>sudo killall -HUP mDNSResponder\n</code></pre> <pre><code>ssh &lt;user&gt;@subnet01.net.dbren.uk\n</code></pre> </li> <li> <p>Run the following commands on the Raspberry Pi to set a static IP address, update the system and expand the filesystem:</p> <pre><code># Become root\nsudo -i\nraspi-config\n# Select \"Advanced Options\" &gt; \"Network Config\" &gt; 2 NetworkManager\n# Select \"Advanced Options\" &gt; \"Expand Filesystem\"\n\n# Set static IP address\nnmcli connection modify eth0 ipv4.method manual ipv4.addresses 192.168.0.2/24 ipv4.gateway 192.168.0.1 ipv4.dns 45.90.28.138,45.90.30.138 connection.autoconnect yes\n\n# Enable connection to apply changes\nnmcli connection up eth0\n\n# Update system\napt-get update &amp;&amp; apt-get upgrade -y\nreboot\n</code></pre> </li> </ol>"},{"location":"infrastructure/secrets-management/","title":"Secrets Management","text":"<p>1Password is used for managing secrets for devices and services deployed in my Homelab.</p>"},{"location":"infrastructure/secrets-management/#integrations","title":"Integrations","text":"<p>1Password has various developer integrations which I use. I also use open source community developed tooling too. These include:</p> <ul> <li>1Password SSH Agent</li> <li>1Password CLI (op)</li> <li>1Password Service Accounts</li> <li>1Password Terraform Provider</li> <li><code>community.general.onepassword</code> Ansible Lookup Plugin</li> </ul> <p>The 1Password Service Accounts are used to authenticate these integrations and are restricted to only read from a specific vault.</p>"},{"location":"infrastructure/tailscale/","title":"Tailscale","text":"<p>What is Tailscale?</p> <p>Tailscale is a VPN service that makes the devices and applications you own accessible anywhere in the world, securely and effortlessly. It enables encrypted point-to-point connections using the open source WireGuard protocol, which means only devices on your private network can communicate with each other.</p> <p>Tailscale is used in my Homelab to remotely access services. I've written a little more about this here.</p> <p>An Ansible playbook is used to install and configure Tailscale on all devices in my Homelab. The configuration for each device is managed via Ansible group and host vars.</p>"},{"location":"infrastructure/tailscale/#tailscale-oauth-client","title":"Tailscale OAuth Client","text":"<p>For devices to authenticate to the Tailnet an OAuth client is required.</p>"},{"location":"infrastructure/tailscale/#dns","title":"DNS","text":"<p>My Tailnet is configured to use NextDNS as the upstream DNS provider. This blocks trackers and ads as well as provide DNS resolution for my Homelab without the maintenance overhead of maintaining my own DNS server.</p>"},{"location":"infrastructure/kubernetes/flux/","title":"Flux","text":"<p>Archived</p> <p>This page has been archived and kept for reference. Some of the links on this page may no longer work.</p> <p>FluxCD is deployed on my Talos Kubernetes node to adopt a GitOps approach to deploying applications on Kubernetes. My GitHub repository is the source of truth for Kubernetes applications I have deployed.</p> <p>What is GitOps?</p> <p>GitOps is a way of managing your infrastructure and applications so that whole system is described declaratively and version controlled (most likely in a Git repository), and having an automated process that ensures that the deployed environment matches the state specified in a repository.</p> <p>Source.</p>"},{"location":"infrastructure/kubernetes/flux/#prerequisite","title":"Prerequisite","text":"<p>Install the <code>flux</code> CLI:</p> <pre><code>brew install fluxcd/tap/flux\n</code></pre>"},{"location":"infrastructure/kubernetes/flux/#deploying-the-flux-controllers","title":"Deploying the Flux Controllers","text":"<p>Abstract</p> <ul> <li>Reference Documentation - GitHub PAT.</li> <li>Reference Documentation - GitHub Personal Account.</li> </ul> <ol> <li> <p>Export the GitHub PAT:</p> <pre><code>export GITHUB_TOKEN=&lt;Insert token here&gt;\n</code></pre> </li> <li> <p>Deploy the Flux controllers:</p> <pre><code>flux bootstrap github \\\n    --token-auth \\\n    --cluster-domain=cluster.net.dbren.uk \\\n    --owner=dbrennand \\\n    --repository=home-ops \\\n    --branch=main \\\n    --path=kubernetes/flux \\\n    --personal\n</code></pre> </li> <li> <p>Verify that the Flux controllers are reconciled and deployed successfully:</p> <pre><code>flux check\n</code></pre> </li> </ol>"},{"location":"infrastructure/kubernetes/flux/#upgrading-flux","title":"Upgrading Flux","text":"<ol> <li> <p>Upgrade the <code>flux</code> CLI:</p> <pre><code>brew install fluxcd/tap/flux\n</code></pre> </li> <li> <p>Update the Flux manifest:</p> <pre><code>flux install --export &gt; ./kubernetes/flux/flux-system/gotk-components.yaml\n</code></pre> </li> <li> <p>Commit and push the changes:</p> <pre><code>git add ./kubernetes/flux/flux-system/gotk-components.yaml\ngit push\n</code></pre> </li> <li> <p>Force flux to upgrade immediately:</p> <pre><code>flux reconcile ks flux-system --with-source\n</code></pre> </li> </ol>"},{"location":"infrastructure/kubernetes/talos/","title":"Talos Linux","text":"<p>Archived</p> <p>This page has been archived and kept for reference. Some of the links on this page may no longer work.</p> <p>In my Homelab I have a single Talos Linux node which runs various Kubernetes workloads.</p> <p>What is Talos Linux?</p> <p>Talos Linux is a Kubernetes optimized Linux distro.</p> <p>It\u2019s designed to be as minimal as possible while still maintaining practicality. For these reasons, Talos has a number of features unique to it:</p> <ol> <li>API managed</li> <li>Immutable file system</li> <li>Minimal packages</li> <li>Secure by default</li> </ol>"},{"location":"infrastructure/kubernetes/talos/#talhelper-machine-configuration","title":"Talhelper &amp; Machine Configuration","text":"<p>Talos Linux's node state is defined by a machine configuration. Machine configuration is applied to the node during initial deployment. The <code>talhelper</code> CLI tool generates the machine configuration declaratively from three files:</p> File Name Description <code>talconfig.yaml</code> Primary configuration file used to generate the machine configuration. <code>talenv.sops.yaml</code> Stores environment variables which are templated using <code>envsubst</code> into the machine configuration. <code>talsecret.sops.yaml</code> Stores secrets used by Talos."},{"location":"infrastructure/kubernetes/talos/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Install the required CLI tools to manage Talos:</p> <pre><code>brew install talhelper siderolabs/tap/sidero-tools kubectl sops age\n</code></pre> </li> <li> <p>Configure <code>sops</code>:</p> <p>Info</p> <p>Referenced Documentation.</p> <pre><code>mkdir -pv ~/.config/sops/age\nmkdir -pv ~/Library/Application Support/sops/age\nage-keygen -o ~/.config/sops/age/keys.txt\nln -s ~/.config/sops/age/keys.txt ~/Library/Application\\ Support/sops/age/keys.txt\n</code></pre> </li> <li> <p>Configure <code>sops</code> for <code>talhelper</code> by adding the age public key to the <code>kubernetes/.sops.yaml</code> file:</p> <pre><code>---\ncreation_rules:\n  - age: &gt;-\n      &lt;age-public-key&gt; # Get this in the keys.txt file from previous step\n</code></pre> </li> </ol>"},{"location":"infrastructure/kubernetes/talos/#talos-iso","title":"Talos ISO","text":"<p>ISO image I used when originally deploying my Talos node.</p>"},{"location":"infrastructure/kubernetes/talos/#deploy-the-talos-iso-on-proxmox","title":"Deploy the Talos ISO on Proxmox","text":"<ol> <li> <p>Download the ISO to the Node 1 <code>local</code> storage.</p> </li> <li> <p>Navigate to <code>proxmox01</code> &gt; <code>Create VM</code>.</p> </li> <li> <p>Provide the following details for <code>General</code> and click Next:</p> Setting Value Name <code>talos01</code> ID 100 Node <code>proxmox01</code> Start at boot \u274c </li> <li> <p>Under <code>OS</code>, select the storage where the ISO was downloaded to and choose the <code>metal-amd64.iso</code>. Click Next.</p> </li> <li> <p>Under <code>System</code>, select the <code>VirtIO SCSI Single</code> controller and <code>Qemu Agent</code> and click Next.</p> </li> <li> <p>Provide the following details for <code>Disks</code> and click Next:</p> Setting Value Bus/Device <code>SCSI</code> Storage <code>lv-ssd-crucial</code> Size <code>100GiB</code> Format <code>Raw disk image (raw)</code> Discard \u2705 SSD Emulation \u2705 IO thread \u2705 Backup \u2705 Setting Value Bus/Device <code>SCSI</code> Storage <code>lv-ssd-crucial</code> Size <code>200GiB</code> Format <code>Raw disk image (raw)</code> Discard \u2705 SSD Emulation \u2705 IO thread \u2705 Backup \u2705 </li> <li> <p>Provide the following details for <code>CPU</code> and click Next:</p> Setting Value Cores <code>4</code> Type <code>host</code> </li> <li> <p>Provide the following details for <code>Memory</code> and click Next:</p> Setting Value Memory (MiB) <code>4096</code> Ballooning Device \u2705 Minimum Memory <code>4096</code> </li> <li> <p>Leave <code>Network</code> as default, click Next and confirm deployment.</p> </li> <li> <p>Start the <code>talos01</code> VM and open the console to begin the installation.</p> </li> <li> <p>In the console, press E to edit the grub menu option and add the end of the kernel boot options enter:</p> <pre><code>ip=192.168.0.10::192.168.0.1:255.255.255.0::eth0:off\n</code></pre> </li> <li> <p>Press CTRL + X to boot.</p> </li> </ol> <p>Talos will now have booted into maintenance mode and is waiting for machine configuration to be applied. It should look like the screenshot below:</p> <p></p>"},{"location":"infrastructure/kubernetes/talos/#deploying-talos","title":"Deploying Talos","text":""},{"location":"infrastructure/kubernetes/talos/#generating-and-applying-the-machine-configuration","title":"Generating and applying the Machine Configuration","text":"<p>Info</p> <p>Referenced Documentation.</p> <p>These steps only need to be performed during the initial set up of Talos and assume you've already created the <code>talconfig.yaml</code>, <code>talenv.sops.yaml</code> and <code>.sops.yaml</code> files.</p> <pre><code>cd kubernetes/talos\ntalhelper gensecret &gt; talsecret.sops.yaml\nsops -e -i talsecret.sops.yaml\nsops -e -i talenv.sops.yaml\ntalhelper genconfig\ntalosctl apply-config --talosconfig=./clusterconfig/talosconfig --nodes=192.168.0.10 --file=./clusterconfig/home-ops-talos01.yaml --insecure\n</code></pre>"},{"location":"infrastructure/kubernetes/talos/#bootstrap-talos","title":"Bootstrap Talos","text":"<ol> <li> <p>Bootstrap etcd:</p> <pre><code>talosctl bootstrap --talosconfig=./clusterconfig/talosconfig --nodes=192.168.0.10\n</code></pre> </li> <li> <p>Retrieve the <code>kubeconfig</code>:</p> <pre><code>talosctl kubeconfig --talosconfig=./clusterconfig/talosconfig --nodes=192.168.0.10\n</code></pre> </li> </ol> <p>Once the machine configuration has finished applying and <code>etcd</code> has finished set up the node should show as below:</p> <p></p>"},{"location":"infrastructure/proxmox/backup/","title":"Proxmox Backup Server (PBS)","text":"<p>What is Proxmox Backup Server?</p> <p>Proxmox Backup Server is an enterprise backup solution, for backing up and restoring VMs, containers, and physical hosts.</p>"},{"location":"infrastructure/proxmox/backup/#deployment","title":"Deployment","text":"<p>PBS is deployed as a virtual machine on the Proxmox VE Node 1. The PBS deployment is via an ISO image.</p> <ol> <li> <p>Download the ISO to the Node 1 storage.</p> </li> <li> <p>Navigate to <code>proxmox01</code> &gt; <code>Create VM</code>.</p> </li> <li> <p>Provide the following details for <code>General</code> and click Next:</p> Setting Value Name <code>backup01</code> Node <code>proxmox01</code> Start at boot \u2705 </li> <li> <p>Under <code>OS</code>, select the storage where the ISO was downloaded to and choose the Proxmox Backup Server ISO image. Click Next.</p> </li> <li> <p>Under <code>System</code>, select the <code>VirtIO SCSI Single</code> controller and click Next.</p> </li> <li> <p>Provide the following details for <code>Disks</code> and click Next:</p> Setting Value Bus/Device <code>SCSI</code> Storage <code>lv-ssd-crucial</code> Size <code>32GiB</code> Format <code>Raw disk image (raw)</code> Discard \u2705 SSD Emulation \u2705 Setting Value Bus/Device <code>SCSI</code> Storage <code>lv-ssd-crucial</code> Size <code>150GiB</code> Format <code>Raw disk image (raw)</code> Discard \u2705 SSD Emulation \u2705 </li> <li> <p>Provide the following details for <code>CPU</code> and click Next:</p> Setting Value Cores <code>4</code> Type <code>host</code> </li> <li> <p>Provide the following details for <code>Memory</code> and click Next:</p> Setting Value Memory (MiB) <code>5120</code> Ballooning Device \u2705 Minimum Memory <code>1024</code> </li> <li> <p>Leave <code>Network</code> as default, click Next and confirm deployment.</p> </li> <li> <p>Start the <code>backup01</code> VM and open the console to begin the installation.</p> </li> <li> <p>Follow the on-screen instructions to install PBS, when prompted enter the following details:</p> Setting Value FQDN <code>backup01.net.dbren.uk</code> Email Enter email Password Enter password Default Gateway <code>192.168.0.1</code> IP Address <code>192.168.0.6/24</code> </li> <li> <p>Once installation has completed, login to the web interface listening on port <code>8007</code> using the FQDN and credentials entered during installation.</p> </li> </ol>"},{"location":"infrastructure/proxmox/backup/#post-installation","title":"Post Installation","text":"<p>Below are the post installation steps for configuring the PBS.</p> <p>Copy SSH public key to the PBS's <code>authorized_keys</code> file:</p> <pre><code>ssh-copy-id root@backup01.net.dbren.uk\n</code></pre>"},{"location":"infrastructure/proxmox/backup/#datastore-creation","title":"Datastore Creation","text":"<p>The PBS requires a datastore to store backups. In my setup, I have two datastores, one which is a Hetzner Storagebox mounted via CIFS on the PBS at <code>/mnt/storagebox</code>, and the other which is a local disk.</p> <p>Documentation</p> <p>Proxmox Backup Server - Backup Storage</p>"},{"location":"infrastructure/proxmox/backup/#hetzner-storagebox-datastore","title":"Hetzner Storagebox Datastore","text":"<ol> <li> <p>Use the <code>playbook-proxmox-backup-cifs.yml</code> to mount the CIFS share on the PBS.</p> </li> <li> <p>SSH to the PBS and create the datastore:</p> <pre><code>proxmox-backup-manager datastore create Remote /mnt/storagebox --gc-schedule \"sun 04:00\"\n</code></pre> </li> </ol>"},{"location":"infrastructure/proxmox/backup/#local-datastore","title":"Local Datastore","text":"<ol> <li> <p>SSH to the PBS and initialise the disk with a GPT:</p> <pre><code>proxmox-backup-manager disk initialize sdb\n</code></pre> </li> <li> <p>Create the datastore:</p> <pre><code>proxmox-backup-manager disk fs create Local --disk sdb --filesystem ext4 --add-datastore true\n</code></pre> </li> <li> <p>Configure the datastore:</p> <pre><code>proxmox-backup-manager datastore update Local --gc-schedule \"sun 04:00\"\n</code></pre> </li> </ol>"},{"location":"infrastructure/proxmox/backup/#verify-job-creation","title":"Verify Job Creation","text":"<p>The verify job is used to verify the integrity of the backups. SSH to the PBS and use the following command to create the verify job:</p> <pre><code>proxmox-backup-manager verify-job create verify-Local --store Local --schedule \"03:00\" --ignore-verified=true --outdated-after=30\n</code></pre>"},{"location":"infrastructure/proxmox/backup/#sync-job-creation","title":"Sync Job Creation","text":"<p>Local &amp; Offsite Copy</p> <p>This makes sure that I have a local copy of backups and an offsite copy on the Hetzner Storagebox.</p> <p>Configure the backups to sync from the <code>Local</code> datastore to <code>Remote</code> datastore:</p> <pre><code>proxmox-backup-manager sync-job create sync-pull-Local-Remote --owner 'root@pam' --store Remote --remote-store Local --schedule \"02:00\" --remove-vanished=true\n</code></pre>"},{"location":"infrastructure/proxmox/backup/#https-web-interface-with-lets-encrypt","title":"HTTPS - Web Interface with Let's Encrypt","text":"<p>Cloudflare API Token &amp; Zone ID</p> <p>See the following instructions for generating a Cloudflare API Token.</p> <p>Furthermore, you will need to obtain your domain's zone ID. This can be found in the Cloudflare dashboard page for the domain, on the right side under API &gt; Zone ID.</p> <ol> <li> <p>Login to the PBS GUI and go to <code>Configuration</code> &gt; <code>Certificates</code> &gt; <code>ACME Accounts</code> &gt; Accounts and click Add:</p> Setting Value Account Name <code>default</code> Email <code>&lt;Email&gt;</code> ACME Directory <code>Let's Encrypt V2</code> Accept TOS <code>True</code> </li> <li> <p>Click Register.</p> </li> <li> <p>Under Challenge Plugins, click Add and enter the following details:</p> Setting Value Plugin ID <code>cloudflare</code> DNS API <code>Cloudflare Managed API</code> CF_Token <code>&lt;Cloudflare API Token&gt;</code> CF_Zone_ID <code>&lt;Cloudflare Zone ID&gt;</code> </li> <li> <p>Click Add.</p> </li> <li> <p>Navigate to <code>Certificates</code> and under ACME click Add:</p> Setting Value Challenge Type <code>DNS</code> Plugin <code>cloudflare</code> Domain <code>backup01.net.dbren.uk</code> </li> <li> <p>Click Create.</p> </li> <li> <p>Under ACME &gt; for <code>Using Account</code> click Edit and select the <code>default</code> account and click Apply.</p> </li> <li> <p>Click Order Certificates Now.</p> </li> </ol> <p>Once completed, the PBS web interface will reload and show the new certificate.</p>"},{"location":"infrastructure/proxmox/backup/#scripts","title":"Scripts","text":"<p>Warning</p> <p>Proceed with caution, before running any of the scripts below, make sure to review the code to ensure it is safe to run.</p> <ol> <li> <p>Run the PBS post install script on PBS:</p> <p>Quote</p> <p>The script will give options to Disable the Enterprise Repo, Add/Correct PBS Sources, Enable the No-Subscription Repo, Add Test Repo, Disable Subscription Nag, Update Proxmox Backup Server and Reboot PBS.</p> <pre><code>bash -c \"$(wget -qLO - https://github.com/tteck/Proxmox/raw/main/misc/post-pbs-install.sh)\"\n</code></pre> </li> </ol>"},{"location":"infrastructure/proxmox/backup/#backup-operations","title":"Backup Operations","text":"<p>Backup operations are performed using the <code>proxmox-backup-client</code> command on the Proxmox VE nodes. Below are some common operations.</p>"},{"location":"infrastructure/proxmox/backup/#view-all-snapshots-for-a-datastore","title":"View all snapshots for a datastore","text":"<pre><code>proxmox-backup-client snapshot list --repository backup01.net.dbren.uk:backup01\n</code></pre>"},{"location":"infrastructure/proxmox/backup/#view-snapshots-for-a-vm","title":"View snapshots for a VM","text":"<pre><code>proxmox-backup-client snapshot list vm/102 --repository backup01.net.dbren.uk:backup01\n</code></pre>"},{"location":"infrastructure/proxmox/backup/#delete-a-snapshot-for-a-vm","title":"Delete a snapshot for a VM","text":"<pre><code>proxmox-backup-client snapshot forget vm/102/2024-06-27T16:57:49Z --repository backup01.net.dbren.uk:backup01\n</code></pre>"},{"location":"infrastructure/proxmox/ve/","title":"Proxmox Virtual Environment (VE)","text":"<p>What is Proxmox VE?</p> <p>Proxmox Virtual Environment is a complete open-source platform for enterprise virtualization. With the built-in web interface you can easily manage VMs and containers, software-defined storage and networking, high-availability clustering, and multiple out-of-the-box tools using a single solution.</p>"},{"location":"infrastructure/proxmox/ve/#proxmox-ve-specs","title":"Proxmox VE Specs","text":""},{"location":"infrastructure/proxmox/ve/#node-1","title":"Node 1","text":"<p>Minisforum Venus Series UN1265</p> Component Details CPU Intel\u00ae Core\u2122 i7-12650H Processor, 10 Cores/16 Threads (24M Cache, up to 4.70 GHz) Memory 64GB DDR4 3200MHz SODIMM (2x32GB) Storage (Internal) Samsung NVMe 970 EVO Plus 1TB Storage (External) Crucial SSD MX500 2TB Storage (External) Samsung SSD 870 QVO 1TB Storage (External) 64GB USB"},{"location":"infrastructure/proxmox/ve/#deployment","title":"Deployment","text":"<ol> <li> <p>Power on the node and enter the BIOS.</p> </li> <li> <p>Go to <code>Advanced</code> &gt; <code>System Devices Configuration</code> and set <code>VT-d</code> and <code>SR-IOV</code> to <code>Enabled</code>.</p> </li> <li> <p>Download the Proxmox VE ISO and flash it to a USB drive using a tool such as Etcher.</p> </li> <li> <p>Insert the USB drive into the node and boot to the USB by pressing the <code>DELETE</code> key during boot.</p> </li> <li> <p>Follow the on-screen instructions to install Proxmox VE, when prompted enter the following details:</p> Setting Value Install Disk <code>/dev/nvme0n1</code> FQDN <code>proxmox01.net.dbren.uk</code> Email Enter email Password Enter password Default Gateway <code>192.168.0.1</code> IP Address <code>192.168.0.4/24</code> </li> <li> <p>Once installation has completed, login to the web interface listening on port <code>8006</code> using the FQDN and credentials entered during installation.</p> </li> </ol>"},{"location":"infrastructure/proxmox/ve/#post-installation","title":"Post Installation","text":"<p>Below are the post installation steps for configuring the Proxmox VE node.</p> <p>Copy SSH public key to the Proxmox VE node's <code>authorized_keys</code> file:</p> <pre><code>ssh-copy-id root@proxmox01.net.dbren.uk\n</code></pre>"},{"location":"infrastructure/proxmox/ve/#storage","title":"Storage","text":"<ol> <li> <p>Extend the Proxmox <code>data</code> logical volume on each node to use the remaining space in the volume group:</p> <pre><code>lvextend -l +100%FREE /dev/pve/data\n</code></pre> </li> <li> <p>Use the <code>playbook-proxmox-storage.yml</code> to configure the Proxmox storage on Node 1.</p> </li> </ol>"},{"location":"infrastructure/proxmox/ve/#https-web-interface-with-lets-encrypt","title":"HTTPS - Web Interface with Let's Encrypt","text":"<p>Cloudflare API Token &amp; Zone ID</p> <p>See the following instructions for generating a Cloudflare API Token.</p> <p>Furthermore, you will need to obtain your domain's zone ID. This can be found in the Cloudflare dashboard page for the domain, on the right side under API &gt; Zone ID.</p> <ol> <li> <p>Login to the Proxmox GUI on Node 1 and go to <code>Datacenter</code> &gt; <code>ACME</code> &gt; Accounts and click Add:</p> Setting Value Account Name <code>default</code> Email <code>&lt;Email&gt;</code> ACME Directory <code>Let's Encrypt V2</code> Accept TOS <code>True</code> </li> <li> <p>Click Register.</p> </li> <li> <p>Under Challenge Plugins, click Add and enter the following details:</p> Setting Value Plugin ID <code>cloudflare</code> DNS API <code>Cloudflare Managed API</code> CF_Token <code>&lt;Cloudflare API Token&gt;</code> CF_Zone_ID <code>&lt;Cloudflare Zone ID&gt;</code> </li> <li> <p>Click Add.</p> </li> <li> <p>Navigate to <code>Datacenter</code> &gt; <code>proxmox01</code> &gt; <code>Certificates</code> and under ACME click Add:</p> Setting Value Challenge Type <code>DNS</code> Plugin <code>cloudflare</code> Domain <code>proxmox01.net.dbren.uk</code> </li> <li> <p>Click Create.</p> </li> <li> <p>Under ACME &gt; for <code>Using Account</code> click Edit and select the <code>default</code> account and click Apply.</p> </li> <li> <p>Click Order Certificates Now.</p> </li> </ol> <p>Once completed, the <code>pveproxy.service</code> will reload the web interface and show the new certificate.</p>"},{"location":"infrastructure/proxmox/ve/#scripts","title":"Scripts","text":"<p>Warning</p> <p>Proceed with caution, before running any of the scripts below, make sure to review the code to ensure it is safe to run.</p> <ol> <li> <p>Run the Proxmox VE post install script on both PVE nodes:</p> <p>Quote</p> <p>This script provides options for managing Proxmox VE repositories, including disabling the Enterprise Repo, adding or correcting PVE sources, enabling the No-Subscription Repo, adding the test Repo, disabling the subscription nag, updating Proxmox VE, and rebooting the system.</p> <pre><code>bash -c \"$(wget -qLO - https://github.com/tteck/Proxmox/raw/main/misc/post-pve-install.sh)\"\n</code></pre> </li> <li> <p>Run the Proxmox Dark Theme script:</p> <p>Quote</p> <p>A dark theme for the Proxmox VE Web UI is a custom theme created by Weilbyte that changes the look and feel of the Proxmox web-based interface to a dark color scheme. This theme can improve the visual experience and make the interface easier on the eyes, especially when used in low-light environments.</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/Weilbyte/PVEDiscordDark/master/PVEDiscordDark.sh ) install\n</code></pre> </li> </ol>"},{"location":"infrastructure/proxmox/ve/#configure-backup-schedules","title":"Configure Backup Schedules","text":"<p>Note</p> <p>The following steps are to be completed once the Proxmox Backup Server has been deployed and configured.</p> <ol> <li> <p>Navigate to the Proxmox GUI on Node 1 and go to <code>Datacenter</code> &gt; <code>Storage</code> &gt; <code>Add</code> and choose <code>Proxmox Backup Server</code>.</p> </li> <li> <p>Enter the following details and click Add:</p> <p>Note</p> <p>The following Proxmox Backup Server datastore was called <code>backup01</code>.</p> Setting Value ID <code>backup01-Remote</code> Server <code>backup01.net.dbren.uk</code> Datastore <code>Remote</code> Username <code>root@pam</code> Password Enter password Fingerprint Copy from Proxmox Backup Server Dashboard &gt; <code>Show Fingerprint</code> Encryption Upload an existing client encryption key </li> <li> <p>Repeat the steps above for <code>Local</code> (formerly <code>backup02</code>):</p> Setting Value ID <code>backup01-Local</code> Server <code>backup01.net.dbren.uk</code> Datastore <code>Local</code> Username <code>root@pam</code> Password Enter password Fingerprint Copy from Proxmox Backup Server Dashboard &gt; <code>Show Fingerprint</code> Encryption Upload an existing client encryption key </li> <li> <p>Navigate to <code>Datacenter</code> &gt; <code>Backup</code> &gt; <code>Add</code> and enter the following details:</p> Setting Value Storage <code>backup01-Local</code> Schedule <code>01:00</code> Selection Mode <code>Exclude selected VMs</code> Mode <code>Snapshot</code> Retention - Keep Daily 10 Retention - Keep Weekly 2 <p>Choose <code>backup01</code> to exclude from backups and click OK.</p> </li> </ol>"},{"location":"infrastructure/proxmox/ve/#archived-steps","title":"Archived Steps","text":"<p>Note</p> <p>The documentation under this heading are old steps used when I had a 2 node Proxmox VE cluster. I've kept them here in case I ever need them again in the future.</p>"},{"location":"infrastructure/proxmox/ve/#create-the-proxmox-cluster","title":"Create the Proxmox Cluster","text":"<ol> <li> <p>Navigate to the Proxmox GUI on Node 1 (Primary) and go to <code>Datacenter</code> &gt; <code>Cluster</code> &gt; <code>Create Cluster</code>:</p> Setting Value Name <code>home-ops</code> </li> <li> <p>Once the cluster has been created, click Join Information and copy the alphanumeric string to the clipboard.</p> </li> <li> <p>Navigate to the Proxmox GUI on Node 2 (Secondary) and go to <code>Datacenter</code> &gt; <code>Cluster</code> &gt; <code>Join Cluster</code> and paste the alphanumeric string into the text box.</p> </li> <li> <p>Enter Node 1's root password for the peer's root password field and click Join 'home-ops'.</p> </li> <li> <p>Wait for the cluster to establish. You will know when this has completed as on each node's GUIs you should now see the other node listed under <code>Datacenter</code>.</p> </li> </ol>"},{"location":"infrastructure/proxmox/ve/#create-external-vote-server","title":"Create External Vote Server","text":"<p>Due to the Proxmox cluster only consisting of two nodes, there is no way to establish quorum.</p> <p>What's Quroum?</p> <p>A quorum is the minimum number of votes that a distributed transaction has to obtain in order to be allowed to perform an operation in a distributed system.</p> <p>Without quorum, if one node goes down, the other node will not be able to determine if it is the only node left in the cluster or if the other node is still running. This is often referred to as a split-brain scenario. Luckily, Proxmox's Corosync supports an external vote server (known as a Corosync Quorum Device (QDevice)) to act as a tie-breaker. This lightweight daemon can be run on a device such as a Raspberry Pi.</p> <ol> <li> <p>On each Proxmox VE cluster node, install <code>corosync-qdevice</code>:</p> <pre><code>apt-get -y install corosync-qdevice\n</code></pre> </li> <li> <p>On the Raspberry Pi, install <code>corosync-qnetd</code>:</p> <pre><code>sudo apt-get -y install corosync-qnetd\n</code></pre> </li> <li> <p>On Proxmox Node 1 (Primary), execute the following command to add the QDevice to the cluster:</p> <pre><code>pvecm qdevice setup 192.168.0.3\n</code></pre> </li> <li> <p>Once added, verify the QDevice is online:</p> <pre><code>pvecm status\n</code></pre> </li> </ol> <p>If the QDevice is successfully added, you should see the following:</p> <pre><code>Membership information\n----------------------\n    Nodeid      Votes    Qdevice Name\n0x00000001          1    A,V,NMW 192.168.0.4 (local)\n0x00000002          1    A,V,NMW 192.168.0.3\n0x00000000          1            Qdevice\n</code></pre>"},{"location":"miscellaneous/nuc-app-config/","title":"NUC - Application Configuration","text":"<p>Note</p> <p>This page has been archived and kept for reference.</p>"},{"location":"miscellaneous/nuc-app-config/#sonarr-radarr","title":"Sonarr &amp;  Radarr","text":"<ol> <li> <p>Go to <code>Settings &gt; Media Management</code> and click Show Advanced.</p> </li> <li> <p>Configure the following settings:</p> <p>Tip</p> <p>Shoutout to TRaSH Guides for the naming formats and other settings:</p> <ul> <li> <p>Radarr</p> </li> <li> <p>Sonarr</p> </li> </ul> Setting Value Rename Episodes / Rename Movies \u2705 Replace Illegal Characters \u2705 Standard Movie Format <code>{Movie CleanTitle} {(Release Year)} [imdbid-{ImdbId}] - {Edition Tags }{[Custom Formats]}{[Quality Full]}{[MediaInfo 3D]}{[MediaInfo VideoDynamicRangeType]}{[Mediainfo AudioCodec}{ Mediainfo AudioChannels}][{Mediainfo VideoCodec}]{-Release Group}</code> Movie Folder Format <code>{Movie CleanTitle} ({Release Year}) [imdbid-{ImdbId}]</code> Standard Episode Format <code>{Series TitleYear} - S{season:00}E{episode:00} - {Episode CleanTitle} [{Preferred Words }{Quality Full}]{[MediaInfo VideoDynamicRangeType]}{[Mediainfo AudioCodec}{ Mediainfo AudioChannels]}{[MediaInfo VideoCodec]}{-Release Group}</code> Daily Episode Format <code>{Series TitleYear} - {Air-Date} - {Episode CleanTitle} [{Preferred Words }{Quality Full}]{[MediaInfo VideoDynamicRangeType]}{[Mediainfo AudioCodec}{ Mediainfo AudioChannels]}{[MediaInfo VideoCodec]}{-Release Group}</code> Anime Episode Format <code>{Series TitleYear} - S{season:00}E{episode:00} - {absolute:000} - {Episode CleanTitle} [{Preferred Words }{Quality Full}]{[MediaInfo VideoDynamicRangeType]}[{MediaInfo VideoBitDepth}bit]{[MediaInfo VideoCodec]}[{Mediainfo AudioCodec} { Mediainfo AudioChannels}]{MediaInfo AudioLanguages}{-Release Group}</code> Series Folder Format <code>{Series TitleYear} [tvdbid-{TvdbId}]</code> Season Folder Format <code>Season {season:00}</code> Multi-Episode Style Prefixed Range Delete empty folders \u2705 Use Hardlinks instead of Copy \u2705 Propers and Repacks Do not Prefer </li> <li> <p>Click Add Root Folder and configure the path to:</p> <p>Sonarr: <code>/data/media/tv</code></p> <p>Radarr: <code>/data/media/movies</code></p> </li> <li> <p>Go to <code>Settings &gt; Quality</code> and set the quality definitions from TRaSH guides:</p> <ul> <li>Radarr</li> <li>Sonarr</li> </ul> </li> <li> <p>Go to <code>Settings &gt; Download Clients</code> and click the <code>+</code> button.</p> </li> <li> <p>Under Torrents select Transmission and enter the following settings:</p> Setting Value Name <code>Transmission</code> Enable \u2705 Host <code>transmission</code> Port <code>9091</code> Remove Completed \u2705 </li> <li> <p>Click Save.</p> </li> <li> <p>Go to <code>Settings &gt; General</code> and under Analytics disable the checkbox.</p> </li> </ol>"},{"location":"miscellaneous/nuc-app-config/#prowlarr","title":"Prowlarr","text":"<ol> <li> <p>Go to <code>Settings &gt; Indexers</code> and click Show Advanced.</p> </li> <li> <p>Click the <code>+</code> button to add an indexer proxy.</p> </li> <li> <p>Select <code>Http</code> and enter the following settings:</p> Setting Value Name <code>Privoxy</code> Tags <code>privoxy</code> Host <code>transmission</code> Port <code>8118</code> </li> <li> <p>Click Save.</p> </li> <li> <p>Go to <code>Settings &gt; Apps</code> and click the <code>+</code> button to add an application.</p> </li> <li> <p>Add two applications for Sonarr and Radarr respectively:</p> Setting Value Name <code>Sonarr</code> Sync Level <code>Full Sync</code> Prowlarr Server <code>https://prowlarr.net.domain.tld</code> Sonarr Server <code>http://sonarr:8989</code> ApiKey <code>Sonarr API key from Settings &gt; General</code> Setting Value Name <code>Radarr</code> Sync Level <code>Full Sync</code> Prowlarr Server <code>https://prowlarr.net.domain.tld</code> Sonarr Server <code>http://radarr:7878</code> ApiKey <code>Radarr API key from Settings &gt; General</code> </li> <li> <p>Click Save.</p> </li> <li> <p>Go to <code>Settings &gt; Notifications</code> and click the <code>+</code> button to add a connection.</p> </li> <li> <p>Select <code>Telegram</code> and enter the following settings:</p> Setting Value Name <code>Telegram</code> Notification Triggers <code>On Health Issue</code>, <code>On Application Update</code> Bot Token <code>Enter Telegram bot token from https://t.me/BotFather</code> Chat ID <code>Enter Telegram chat ID from https://t.me/userinfobot</code> </li> <li> <p>Click Save.</p> </li> <li> <p>Go to <code>Indexers</code> and click Add Indexer.</p> </li> <li> <p>Select an indexer from the list and when configuring, make sure to add the <code>privoxy</code> tag so traffic is routed through the proxy.</p> </li> </ol>"},{"location":"miscellaneous/nuc-app-config/#jellyfin","title":"Jellyfin","text":"<ol> <li> <p>Login to Jellyfin and under Administration go to <code>Dashboard &gt; Libraries</code>.</p> </li> <li> <p>Click Add Media Library and add one for movies and shows respectively:</p> <p>Movies: <code>/data/media/movies</code></p> <p>Shows: <code>/data/media/tv</code></p> </li> <li> <p>Under <code>Playback</code> select <code>Intel QuickSync (QSV)</code> as the hardware acceleration option.</p> </li> <li> <p>Check Throttle Transcodes and click Save.</p> </li> <li> <p>Under <code>Networking</code> check <code>Allow remote connections to this server</code> and click Save.</p> </li> </ol> <p>Enjoy! \u2728\ud83d\ude80</p>"},{"location":"miscellaneous/python-uv/","title":"Python","text":"<p>uv is used to to manage Python installations and virtual environments on my M1 Pro Max MacBook Pro.</p> <p>Install <code>uv</code> via homebrew:</p> <pre><code>brew install uv\n</code></pre> <p>Then, install Python using <code>uv</code>:</p> <pre><code>uv python install 3.13\n</code></pre>"}]}